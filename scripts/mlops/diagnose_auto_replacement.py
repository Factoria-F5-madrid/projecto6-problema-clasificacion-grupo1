#!/usr/bin/env python3
"""
Script de diagnÃ³stico para el sistema de Auto-reemplazo de Modelos
FIX: MÃ©tricas de modelos en 0.0
"""

import os
import sys
import json
from datetime import datetime

# AÃ±adir el path del backend
sys.path.append(os.path.join(os.path.dirname(__file__), '..', '..', 'backend'))

from mlops.auto_model_replacement import AutoModelReplacement

def diagnose_models():
    """Diagnosticar el estado de los modelos"""
    print("ðŸ” DIAGNÃ“STICO DEL SISTEMA DE AUTO-REEMPLAZO")
    print("=" * 60)
    
    replacement_system = AutoModelReplacement()
    
    # Ver estado actual
    print("\nðŸ“Š Estado cargado:")
    print(f"   - Modelos candidatos: {len(replacement_system.candidate_models)}")
    print(f"   - Evaluaciones: {len(replacement_system.evaluation_history)}")
    print(f"   - Modelo actual: {replacement_system.current_model['name'] if replacement_system.current_model else 'None'}")
    
    # Verificar cada modelo
    print("\nðŸ” ANÃLISIS DE MODELOS:")
    for i, model in enumerate(replacement_system.candidate_models):
        print(f"\nðŸ“‹ Modelo {i+1}: {model['name']}")
        print(f"   - Estado: {model['status']}")
        print(f"   - Tipo: {model['type']}")
        print(f"   - Path: {model['model_path']}")
        print(f"   - Existe archivo: {os.path.exists(model['model_path'])}")
        
        metrics = model.get('performance_metrics', {})
        print(f"   - MÃ©tricas:")
        print(f"     â€¢ Score: {metrics.get('avg_overall_score', 0):.3f}")
        print(f"     â€¢ Accuracy: {metrics.get('avg_accuracy', 0):.3f}")
        print(f"     â€¢ Precision: {metrics.get('avg_precision', 0):.3f}")
        print(f"     â€¢ Recall: {metrics.get('avg_recall', 0):.3f}")
        print(f"     â€¢ F1: {metrics.get('avg_f1_score', 0):.3f}")
        print(f"     â€¢ Evaluaciones: {metrics.get('total_evaluations', 0)}")
        
        evals = model.get('evaluations', [])
        print(f"   - Historial de evaluaciones: {len(evals)}")
        if evals:
            print(f"     â€¢ Ãšltima evaluaciÃ³n: {evals[-1].get('timestamp', 'N/A')}")
            print(f"     â€¢ Ãšltimo score: {evals[-1].get('overall_score', 0):.3f}")
    
    # Verificar archivos de estado
    print("\nðŸ“ ARCHIVOS DE ESTADO:")
    state_path = "backend/mlops/replacement_results/system_state.json"
    if os.path.exists(state_path):
        print(f"   âœ… system_state.json existe")
        with open(state_path, 'r') as f:
            state = json.load(f)
        print(f"   - Modelos en archivo: {len(state.get('candidate_models', []))}")
        print(f"   - Evaluaciones en archivo: {len(state.get('evaluation_history', []))}")
    else:
        print(f"   âŒ system_state.json NO existe")
    
    # Verificar modelos demo
    print("\nðŸ¤– MODELOS DEMO:")
    demo_models = [
        "backend/models/saved/demo_model_a.pkl",
        "backend/models/saved/demo_model_b.pkl",
        "backend/models/saved/demo_vectorizer.pkl"
    ]
    for model_path in demo_models:
        exists = os.path.exists(model_path)
        print(f"   {'âœ…' if exists else 'âŒ'} {model_path}")
    
    print("\n" + "=" * 60)
    
    # Recomendaciones
    print("\nðŸ’¡ RECOMENDACIONES:")
    
    # Verificar si hay modelos con score 0
    zero_score_models = [m for m in replacement_system.candidate_models 
                        if m.get('performance_metrics', {}).get('avg_overall_score', 0) == 0]
    
    if zero_score_models:
        print("   âš ï¸  Hay modelos con score 0.0:")
        for model in zero_score_models:
            print(f"      - {model['name']}: {model.get('performance_metrics', {}).get('avg_overall_score', 0):.3f}")
        print("   ðŸ”§ SoluciÃ³n: Ejecutar evaluaciones para actualizar mÃ©tricas")
    
    # Verificar si hay duplicados
    model_names = [m['name'] for m in replacement_system.candidate_models]
    duplicates = set([name for name in model_names if model_names.count(name) > 1])
    if duplicates:
        print(f"   âš ï¸  Hay modelos duplicados: {duplicates}")
        print("   ðŸ”§ SoluciÃ³n: El sistema deberÃ­a limpiar automÃ¡ticamente")
    
    # Verificar si el modelo actual tiene score 0
    if replacement_system.current_model:
        current_score = replacement_system.current_model.get('performance_metrics', {}).get('avg_overall_score', 0)
        if current_score == 0:
            print(f"   âš ï¸  Modelo actual tiene score 0.0")
            print("   ðŸ”§ SoluciÃ³n: Establecer un modelo con score > 0")
    
    print("\nâœ… DiagnÃ³stico completado")

if __name__ == "__main__":
    diagnose_models()
