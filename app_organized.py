"""
App Organizada - Detector de Hate Speech
Versi√≥n con men√∫ para separar funciones b√°sicas y avanzadas
"""

import streamlit as st
import pandas as pd
import numpy as np
import pickle
import os
import sys
from PIL import Image

# Add backend to path
sys.path.append('backend')

# Import our systems
from models.final_smart_selector import FinalSmartSelector
from models.improved_smart_selector import ImprovedSmartSelector
from models.advanced_hybrid_system import AdvancedHybridSystem
from models.ultimate_hybrid_system import UltimateHybridSystem

# Import MLOps components
from mlops.ab_testing import ABTestingSystem

# Page config
st.set_page_config(
    page_title='Detector de Hate Speech',
    page_icon='üö®',
    layout='wide',
    initial_sidebar_state='expanded'
)

# Load systems
@st.cache_resource
def load_systems():
    """Carga todos los sistemas disponibles"""
    try:
        systems = {
            'Final': FinalSmartSelector(),
            'Improved': ImprovedSmartSelector(),
            'Advanced': AdvancedHybridSystem()
        }
        return systems
    except Exception as e:
        st.error(f"Error cargando sistemas: {e}")
        return None

def main():
    """Funci√≥n principal de la app"""
    
    # Sidebar menu
    st.sidebar.title("üö® Detector de Hate Speech")
    st.sidebar.markdown("**Sistema h√≠brido inteligente**")
    
    # Menu options
    menu = st.sidebar.selectbox(
        "Selecciona una funci√≥n:",
        [
            "üè† Detector Principal",
            "üìä Comparaci√≥n de Modelos", 
            "üß™ Casos de Prueba",
            "üìà M√©tricas del Sistema",
            "üî¨ A/B Testing (MLOps)",
            "‚öôÔ∏è Configuraci√≥n Avanzada"
        ]
    )
    
    # Load systems
    with st.spinner("üîÑ Cargando sistemas..."):
        systems = load_systems()
    
    if systems is None:
        st.error("‚ùå No se pudieron cargar los sistemas")
        return
    
    # Load A/B Testing system
    @st.cache_resource
    def load_ab_system():
        return ABTestingSystem()
    
    ab_system = load_ab_system()
    
    # Route to selected page
    if menu == "üè† Detector Principal":
        detector_page(systems['Final'])
    elif menu == "üìä Comparaci√≥n de Modelos":
        comparison_page(systems)
    elif menu == "üß™ Casos de Prueba":
        test_page(systems['Final'])
    elif menu == "üìà M√©tricas del Sistema":
        metrics_page()
    elif menu == "üî¨ A/B Testing (MLOps)":
        ab_testing_page(systems, ab_system)
    elif menu == "‚öôÔ∏è Configuraci√≥n Avanzada":
        config_page()

def detector_page(system):
    """P√°gina principal del detector"""
    st.header("üîç Detector Principal")
    st.markdown("**Sistema Final - M√°xima Precisi√≥n (81.2%)**")
    
    # Input
    text_input = st.text_area(
        "Ingresa el texto a analizar:",
        placeholder="Escribe aqu√≠ el texto que quieres analizar...",
        height=100
    )
    
    # Analyze button
    if st.button("üöÄ Analizar", type="primary", use_container_width=True):
        if text_input.strip():
            analyze_text(system, text_input)
        else:
            st.warning("‚ö†Ô∏è Por favor, ingresa alg√∫n texto para analizar")

def comparison_page(systems):
    """P√°gina de comparaci√≥n de modelos"""
    st.header("üìä Comparaci√≥n de Modelos")
    st.markdown("**Compara el rendimiento de diferentes sistemas**")
    
    # Input
    text_input = st.text_area(
        "Ingresa el texto a comparar:",
        placeholder="Escribe aqu√≠ el texto para comparar entre modelos...",
        height=100
    )
    
    if st.button("üîÑ Comparar Modelos", type="primary"):
        if text_input.strip():
            compare_models(systems, text_input)
        else:
            st.warning("‚ö†Ô∏è Por favor, ingresa alg√∫n texto para comparar")

def test_page(system):
    """P√°gina de casos de prueba"""
    st.header("üß™ Casos de Prueba")
    st.markdown("**Prueba el sistema con casos predefinidos**")
    
    test_cases = [
        ("fuck you", "Offensive Language"),
        ("Hello, how are you?", "Neither"),
        ("Women are inferior to men", "Hate Speech"),
        ("This is fucking amazing", "Neither"),
        ("You are a jerk", "Offensive Language"),
        ("That's brilliant work", "Neither"),
        ("F*ck you", "Offensive Language"),
        ("@sshole", "Offensive Language")
    ]
    
    cols = st.columns(2)
    for i, (text, expected) in enumerate(test_cases):
        with cols[i % 2]:
            if st.button(f"üìù {text}", key=f"test_{i}", use_container_width=True):
                analyze_text(system, text)

def metrics_page():
    """P√°gina de m√©tricas del sistema"""
    st.header("üìà M√©tricas del Sistema")
    st.markdown("**Rendimiento y estad√≠sticas del sistema**")
    
    # System performance
    col1, col2, col3 = st.columns(3)
    
    with col1:
        st.metric("Precisi√≥n General", "81.2%", "2.1%")
    
    with col2:
        st.metric("Hate Speech", "90.0%", "5.2%")
    
    with col3:
        st.metric("Offensive Language", "80.0%", "3.1%")
    
    # Performance chart
    st.subheader("üìä Distribuci√≥n de Clasificaciones")
    
    data = {
        'Categor√≠a': ['Hate Speech', 'Offensive Language', 'Neither'],
        'Precisi√≥n': [90.0, 80.0, 75.0],
        'Recall': [85.0, 82.0, 78.0],
        'F1-Score': [87.5, 81.0, 76.5]
    }
    
    df = pd.DataFrame(data)
    st.bar_chart(df.set_index('Categor√≠a'))

def config_page():
    """P√°gina de configuraci√≥n avanzada"""
    st.header("‚öôÔ∏è Configuraci√≥n Avanzada")
    st.markdown("**Ajustes y par√°metros del sistema**")
    
    st.subheader("üîß Par√°metros del Sistema")
    
    col1, col2 = st.columns(2)
    
    with col1:
        st.slider("Umbral de Confianza", 0.0, 1.0, 0.5, 0.1)
        st.slider("Peso de Reglas", 0.0, 1.0, 0.6, 0.1)
    
    with col2:
        st.slider("Peso de ML", 0.0, 1.0, 0.4, 0.1)
        st.checkbox("Modo Estricto", value=False)
    
    st.subheader("üìä Informaci√≥n del Sistema")
    st.info("""
    **Sistema H√≠brido Final:**
    - 3 modelos ML (Logistic Regression, XGBoost, Random Forest)
    - Reglas inteligentes para palabras ofensivas
    - Detecci√≥n de contexto y evasiones
    - Precisi√≥n: 81.2%
    """)

def analyze_text(system, text):
    """Analiza el texto y muestra los resultados"""
    
    with st.spinner("üîÑ Analizando..."):
        result = system.predict(text)
    
    # Results
    st.markdown("---")
    st.header("üìä Resultados del An√°lisis")
    
    # Classification
    classification = result['prediction']
    confidence = result['confidence']
    method = result['method']
    
    # Color coding
    if classification == "Hate Speech":
        color = "üî¥"
        bg_color = "#ffebee"
    elif classification == "Offensive Language":
        color = "üü°"
        bg_color = "#fff3e0"
    else:
        color = "üü¢"
        bg_color = "#e8f5e8"
    
    # Display result
    col1, col2, col3 = st.columns(3)
    
    with col1:
        st.metric("Clasificaci√≥n", f"{color} {classification}")
    
    with col2:
        st.metric("Confianza", f"{confidence:.1%}")
    
    with col3:
        st.metric("M√©todo", method.replace('_', ' ').title())
    
    # Explanation
    st.markdown("**üí° Explicaci√≥n:**")
    st.info(result['explanation'])

def compare_models(systems, text):
    """Compara el texto entre diferentes modelos"""
    
    st.markdown("---")
    st.header("üìä Comparaci√≥n de Modelos")
    
    results = {}
    
    for name, system in systems.items():
        with st.spinner(f"üîÑ Analizando con {name}..."):
            try:
                result = system.predict(text)
                results[name] = result
            except Exception as e:
                st.error(f"Error con {name}: {e}")
                results[name] = None
    
    # Display comparison
    if results:
        comparison_data = []
        
        for name, result in results.items():
            if result:
                comparison_data.append({
                    'Modelo': name,
                    'Clasificaci√≥n': result['prediction'],
                    'Confianza': f"{result['confidence']:.1%}",
                    'M√©todo': result['method'].replace('_', ' ').title()
                })
        
        if comparison_data:
            df = pd.DataFrame(comparison_data)
            st.dataframe(df, use_container_width=True)
            
            # Agreement analysis
            classifications = [r['prediction'] for r in results.values() if r]
            if len(set(classifications)) == 1:
                st.success("‚úÖ Todos los modelos est√°n de acuerdo")
            else:
                st.warning("‚ö†Ô∏è Los modelos no est√°n de acuerdo")

def ab_testing_page(systems, ab_system):
    """P√°gina de A/B Testing para MLOps"""
    st.header("üî¨ A/B Testing (MLOps)")
    st.markdown("**Nivel Experto - Comparaci√≥n de Modelos en Producci√≥n**")
    
    # Tabs para diferentes funcionalidades
    tab1, tab2, tab3, tab4 = st.tabs(["üöÄ Iniciar Test", "üß™ Prueba en Vivo", "üìä Ver Resultados", "üìà An√°lisis"])
    
    with tab1:
        st.subheader("üöÄ Iniciar Nuevo A/B Test")
        
        col1, col2 = st.columns(2)
        
        with col1:
            st.markdown("**Modelo A (Control)**")
            model_a_name = st.selectbox(
                "Seleccionar Modelo A:",
                ["UltimateHybrid", "FinalSmartSelector", "ImprovedSmartSelector", "AdvancedHybrid"],
                key="model_a"
            )
            
            # Obtener modelo real
            model_a = systems.get('Final')  # Por defecto UltimateHybrid
        
        with col2:
            st.markdown("**Modelo B (Variante)**")
            model_b_name = st.selectbox(
                "Seleccionar Modelo B:",
                ["FinalSmartSelector", "ImprovedSmartSelector", "AdvancedHybrid", "UltimateHybrid"],
                key="model_b"
            )
            
            # Obtener modelo real
            model_b = systems.get('Improved')  # Por defecto FinalSmartSelector
        
        # Configuraci√≥n del test
        st.markdown("**‚öôÔ∏è Configuraci√≥n del Test**")
        
        col1, col2 = st.columns(2)
        
        with col1:
            traffic_split = st.slider(
                "Divisi√≥n de Tr√°fico (%)",
                min_value=10, max_value=90, value=50,
                help="Porcentaje de tr√°fico para el Modelo A"
            )
        
        with col2:
            test_duration = st.number_input(
                "Duraci√≥n (d√≠as)",
                min_value=1, max_value=30, value=7,
                help="D√≠as que durar√° el test"
            )
        
        # Iniciar test
        if st.button("üöÄ Iniciar A/B Test", type="primary"):
            if model_a_name == model_b_name:
                st.error("‚ùå Los modelos deben ser diferentes")
            else:
                with st.spinner("üîÑ Iniciando A/B Test..."):
                    # Actualizar configuraci√≥n
                    ab_system.traffic_split = traffic_split / 100
                    
                    # Iniciar test
                    test_id = ab_system.start_ab_test(
                        model_a_name, model_b_name, 
                        model_a, model_b, test_duration
                    )
                    
                    st.success(f"‚úÖ A/B Test iniciado: `{test_id}`")
                    st.info(f"üìä Divisi√≥n: {traffic_split}% para {model_a_name}, {100-traffic_split}% para {model_b_name}")
                    
                    # Guardar test_id en session state
                    st.session_state.current_test_id = test_id
    
    with tab2:
        st.subheader("üß™ Prueba en Vivo")
        st.markdown("**Haz predicciones en tiempo real para generar datos del A/B test**")
        
        # Verificar si hay un test activo
        if 'current_test_id' not in st.session_state:
            st.warning("‚ö†Ô∏è No hay un test A/B activo. Ve a la pesta√±a 'Iniciar Test' para crear uno.")
        else:
            test_id = st.session_state.current_test_id
            
            # Mostrar informaci√≥n del test activo
            st.info(f"üîÑ Test activo: `{test_id}`")
            
            # Input para texto
            text_input = st.text_area(
                "Ingresa el texto para analizar:",
                placeholder="Escribe aqu√≠ el texto que quieres analizar...",
                height=100,
                key="ab_test_input"
            )
            
            # Bot√≥n para analizar
            if st.button("üîç Analizar con A/B Test", type="primary"):
                if text_input.strip():
                    # Obtener modelos del test
                    import os
                    import json
                    
                    config_path = os.path.join("backend/mlops/ab_results", f"{test_id}_config.json")
                    
                    if os.path.exists(config_path):
                        with open(config_path, 'r') as f:
                            config = json.load(f)
                        
                        model_a_name = config['model_a']['name']
                        model_b_name = config['model_b']['name']
                        
                        # Asignar tr√°fico
                        variant = ab_system.assign_traffic(test_id, f"user_{len(text_input)}")
                        
                        # Obtener modelo correspondiente
                        if variant == 'A':
                            model = systems.get('Final')  # UltimateHybrid
                            model_name = model_a_name
                        else:
                            model = systems.get('Improved')  # FinalSmartSelector
                            model_name = model_b_name
                        
                        # Hacer predicci√≥n
                        import time
                        start_time = time.time()
                        result = model.predict(text_input)
                        response_time = time.time() - start_time
                        
                        # Log predicci√≥n
                        ab_system.log_prediction(
                            test_id, variant, text_input, 
                            result['prediction'], result['confidence'],
                            None, response_time  # No tenemos etiqueta real
                        )
                        
                        # Mostrar resultados
                        st.success(f"‚úÖ Predicci√≥n registrada en el A/B test")
                        
                        col1, col2, col3 = st.columns(3)
                        
                        with col1:
                            st.metric("Modelo Usado", f"{model_name} ({variant})")
                        
                        with col2:
                            st.metric("Predicci√≥n", result['prediction'])
                        
                        with col3:
                            st.metric("Confianza", f"{result['confidence']:.1%}")
                        
                        st.markdown("**üí° Explicaci√≥n:**")
                        st.info(result['explanation'])
                        
                        # Mostrar estad√≠sticas del test
                        st.markdown("---")
                        st.markdown("**üìä Estad√≠sticas del Test:**")
                        
                        # Contar predicciones por modelo
                        log_path = os.path.join("backend/mlops/ab_results", f"{test_id}_logs.jsonl")
                        if os.path.exists(log_path):
                            with open(log_path, 'r') as f:
                                logs = [json.loads(line.strip()) for line in f]
                            
                            model_a_count = len([log for log in logs if log['model_variant'] == 'A'])
                            model_b_count = len([log for log in logs if log['model_variant'] == 'B'])
                            
                            col1, col2 = st.columns(2)
                            
                            with col1:
                                st.metric(f"Predicciones {model_a_name}", model_a_count)
                            
                            with col2:
                                st.metric(f"Predicciones {model_b_name}", model_b_count)
                        
                    else:
                        st.error("‚ùå No se encontr√≥ la configuraci√≥n del test")
                else:
                    st.warning("‚ö†Ô∏è Por favor, ingresa alg√∫n texto para analizar")
    
    with tab3:
        st.subheader("üìä Resultados del A/B Test")
        
        # Listar tests disponibles
        import os
        import json
        from datetime import datetime
        
        results_dir = "backend/mlops/ab_results"
        if os.path.exists(results_dir):
            test_files = [f for f in os.listdir(results_dir) if f.endswith('_config.json')]
            
            if test_files:
                st.markdown("**Tests Disponibles:**")
                
                for test_file in test_files:
                    test_id = test_file.replace('_config.json', '')
                    
                    # Cargar configuraci√≥n
                    with open(os.path.join(results_dir, test_file), 'r') as f:
                        config = json.load(f)
                    
                    col1, col2, col3 = st.columns([3, 2, 1])
                    
                    with col1:
                        st.write(f"**{test_id}**")
                        st.caption(f"A: {config['model_a']['name']} | B: {config['model_b']['name']}")
                    
                    with col2:
                        st.write(f"Estado: {config['status']}")
                        st.caption(f"Inicio: {config['start_time'][:10]}")
                    
                    with col3:
                        if st.button("Ver", key=f"view_{test_id}"):
                            st.session_state.current_test_id = test_id
                
                # Mostrar resultados del test seleccionado
                if 'current_test_id' in st.session_state:
                    test_id = st.session_state.current_test_id
                    
                    st.markdown("---")
                    st.markdown(f"**Resultados de: {test_id}**")
                    
                    # Obtener resultados
                    results = ab_system.get_test_results(test_id)
                    
                    if 'error' in results:
                        st.error(f"‚ùå {results['error']}")
                    else:
                        # M√©tricas generales
                        col1, col2, col3 = st.columns(3)
                        
                        with col1:
                            st.metric("Total Predicciones", results['total_predictions'])
                        
                        with col2:
                            st.metric("Modelo A - Accuracy", 
                                    f"{results['model_a'].get('accuracy', 0):.3f}")
                        
                        with col3:
                            st.metric("Modelo B - Accuracy", 
                                    f"{results['model_b'].get('accuracy', 0):.3f}")
                        
                        # Significancia estad√≠stica
                        significance = results['statistical_significance']
                        
                        if significance['status'] == 'sufficient_data':
                            st.markdown("**üìà Significancia Estad√≠stica:**")
                            
                            col1, col2 = st.columns(2)
                            
                            with col1:
                                st.write(f"**Diferencia:** {significance['difference']:.3f}")
                                st.write(f"**Z-Score:** {significance['z_score']:.3f}")
                            
                            with col2:
                                st.write(f"**Significativo:** {'‚úÖ S√≠' if significance['is_significant'] else '‚ùå No'}")
                                st.write(f"**Ganador:** {significance['winner']}")
                        else:
                            st.warning(f"‚ö†Ô∏è {significance['message']}")
                        
                        # Recomendaci√≥n
                        recommendation = ab_system.get_recommendation(test_id)
                        
                        st.markdown("**üéØ Recomendaci√≥n:**")
                        if recommendation['recommendation'] == 'continue_testing':
                            st.warning(f"‚ö†Ô∏è {recommendation['message']}")
                        elif recommendation['recommendation'] == 'keep_current':
                            st.info(f"‚ÑπÔ∏è {recommendation['message']}")
                        else:
                            st.success(f"‚úÖ {recommendation['message']}")
            else:
                st.info("‚ÑπÔ∏è No hay tests A/B disponibles. Inicia uno en la pesta√±a 'Iniciar Test'")
        else:
            st.info("‚ÑπÔ∏è No hay directorio de resultados. Inicia un test primero.")
    
    with tab4:
        st.subheader("üìà An√°lisis de A/B Testing")
        
        st.markdown("""
        **¬øQu√© es A/B Testing en MLOps?**
        
        A/B Testing es una t√©cnica fundamental en MLOps que permite:
        
        - **üî¨ Comparar modelos** en producci√≥n de forma segura
        - **üìä Medir impacto** de nuevos modelos con datos reales
        - **üìà Optimizar rendimiento** bas√°ndose en m√©tricas objetivas
        - **üõ°Ô∏è Reducir riesgos** al desplegar cambios gradualmente
        
        **M√©tricas que evaluamos:**
        - Accuracy, Precision, Recall, F1-Score
        - Tiempo de respuesta
        - Confianza promedio
        - Significancia estad√≠stica
        
        **Flujo de trabajo:**
        1. **Iniciar test** con dos modelos diferentes
        2. **Dividir tr√°fico** (ej: 50% cada modelo)
        3. **Recopilar m√©tricas** durante el per√≠odo de prueba
        4. **Analizar resultados** con significancia estad√≠stica
        5. **Tomar decisi√≥n** basada en evidencia
        """)
        
        # Gr√°fico de ejemplo
        import plotly.graph_objects as go
        
        # Datos de ejemplo
        days = list(range(1, 8))
        model_a_accuracy = [0.92, 0.93, 0.91, 0.94, 0.92, 0.93, 0.94]
        model_b_accuracy = [0.89, 0.90, 0.88, 0.91, 0.89, 0.90, 0.91]
        
        fig = go.Figure()
        
        fig.add_trace(go.Scatter(
            x=days, y=model_a_accuracy,
            mode='lines+markers',
            name='Modelo A (Control)',
            line=dict(color='blue', width=3)
        ))
        
        fig.add_trace(go.Scatter(
            x=days, y=model_b_accuracy,
            mode='lines+markers',
            name='Modelo B (Variante)',
            line=dict(color='red', width=3)
        ))
        
        fig.update_layout(
            title="Evoluci√≥n de Accuracy en A/B Test",
            xaxis_title="D√≠as",
            yaxis_title="Accuracy",
            hovermode='x unified'
        )
        
        st.plotly_chart(fig, use_container_width=True)

if __name__ == "__main__":
    main()
