# ğŸš€ ML Optimization Framework

## ğŸ“‹ DescripciÃ³n del Proyecto

Este framework estÃ¡ diseÃ±ado para **optimizar modelos de Machine Learning** mediante validaciÃ³n cruzada y ajuste de hiperparÃ¡metros. Es **compatible** con cualquier modelo que crees.

## ğŸ‘¥ DivisiÃ³n de Trabajo

### **CompaÃ±ero B (Ciprian) - Modelado Base:**
- Crea modelos iniciales (Logistic Regression, Naive Bayes, SVM, etc.)
- Hace evaluaciÃ³n bÃ¡sica con mÃ©tricas simples
- **Entrega:** Modelos entrenados + mÃ©tricas bÃ¡sicas

### **CompaÃ±ero C (Barb) - OptimizaciÃ³n:**
- Toma los modelos base de Ciprian
- Aplica validaciÃ³n cruzada y optimizaciÃ³n de hiperparÃ¡metros
- Controla el overfitting (<5%)
- **Entrega:** Modelos optimizados + control de overfitting

## ğŸ“ Estructura del Proyecto

```
backend/
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ cross_validation.py      # âœ… Sistema de validaciÃ³n cruzada
â”‚   â”œâ”€â”€ hyperparameter_tuning.py # ğŸ”„ Framework de optimizaciÃ³n (prÃ³ximo)
â”‚   â”œâ”€â”€ model_evaluation.py      # ğŸ“Š EvaluaciÃ³n detallada (prÃ³ximo)
â”‚   â””â”€â”€ automation_pipeline.py   # ğŸ¤– Pipeline automÃ¡tico (prÃ³ximo)
â”œâ”€â”€ config/
â”‚   â””â”€â”€ model_config.py          # âœ… Configuraciones de modelos
â””â”€â”€ utils/
    â”œâ”€â”€ data_preprocessing.py    # ğŸ”„ Preprocesamiento genÃ©rico (prÃ³ximo)
    â””â”€â”€ experiment_logger.py     # ğŸ“ Logging de experimentos (prÃ³ximo)
```

## âœ… **Archivos Completados:**

### 1. `config/model_config.py`
**Â¿QuÃ© hace?**
- Define 7 algoritmos de ML listos para usar
- Contiene hiperparÃ¡metros predefinidos para cada modelo
- ConfiguraciÃ³n de validaciÃ³n cruzada

**Â¿CÃ³mo lo usa Ciprian?**
```python
from backend.config.model_config import ModelConfig

# Obtener un modelo
model = ModelConfig.get_model('random_forest')

# Obtener hiperparÃ¡metros para optimizaciÃ³n
params = ModelConfig.get_hyperparameters('random_forest')
```

### 2. `models/cross_validation.py`
**Â¿QuÃ© hace?**
- Sistema de validaciÃ³n cruzada (K-Fold, Stratified K-Fold)
- Control automÃ¡tico de overfitting (<5%)
- MÃ©tricas completas: accuracy, precision, recall, F1, ROC-AUC
- ComparaciÃ³n automÃ¡tica de modelos

**Â¿CÃ³mo lo usa Ciprian?**
```python
from backend.models.cross_validation import CrossValidator

# Crear validador
validator = CrossValidator(cv_folds=5)

# Evaluar un modelo
results = validator.evaluate_model(model, X, y)

# Comparar mÃºltiples modelos
comparison = validator.compare_models(models_dict, X, y)
```

### 3. `models/hyperparameter_tuning.py` âœ… **NUEVO**
**Â¿QuÃ© hace?**
- **GridSearchCV**: BÃºsqueda exhaustiva de hiperparÃ¡metros (mÃ¡s lento, mÃ¡s preciso)
- **RandomizedSearchCV**: BÃºsqueda aleatoria de hiperparÃ¡metros (mÃ¡s rÃ¡pido)
- **ComparaciÃ³n**: Compara ambos mÃ©todos automÃ¡ticamente
- **Compatible** con cualquier modelo de sklearn

**Â¿CÃ³mo lo usa Ciprian?**
```python
from backend.models.hyperparameter_tuning import HyperparameterTuner
from backend.config.model_config import ModelConfig

# Crear tuner
tuner = HyperparameterTuner(cv_folds=5)

# Obtener modelo y parÃ¡metros
model = ModelConfig.get_model('random_forest')
params = ModelConfig.get_hyperparameters('random_forest')

# OpciÃ³n 1: Grid Search (mÃ¡s lento, mÃ¡s preciso)
grid_results = tuner.grid_search(model, params, X, y)

# OpciÃ³n 2: Randomized Search (mÃ¡s rÃ¡pido)
random_results = tuner.randomized_search(model, params, X, y, n_iter=100)

# OpciÃ³n 3: Comparar ambos mÃ©todos
comparison = tuner.compare_methods(model, params, X, y)
```

**Â¿CuÃ¡ndo usar cada mÃ©todo?**
- **GridSearchCV**: Cuando tienes pocos hiperparÃ¡metros y tiempo suficiente
- **RandomizedSearchCV**: Cuando tienes muchos hiperparÃ¡metros o poco tiempo
- **compare_methods()**: Para decidir cuÃ¡l mÃ©todo usar en tu caso especÃ­fico

## ğŸ§ª **Testing the Framework:**

Para probar que todo funciona correctamente:

```bash
# Desde la raÃ­z del proyecto
python backend/tests/test_simple.py
```

Este test verifica:
- âœ… CrossValidator funciona correctamente
- âœ… HyperparameterTuner (GridSearch y RandomizedSearch) funciona
- âœ… Control de overfitting funciona
- âœ… Todos los imports y dependencias estÃ¡n correctos

## ğŸ”„ **PrÃ³ximos Pasos:**

1. âœ… **HyperparameterTuner** - OptimizaciÃ³n automÃ¡tica de hiperparÃ¡metros
2. **ModelEvaluator** - EvaluaciÃ³n detallada con visualizaciones
3. **DataPreprocessor** - Preprocesamiento genÃ©rico de datos
4. **ExperimentLogger** - Sistema de logging de experimentos
5. **MLPipeline** - Pipeline completo de automatizaciÃ³n

## ğŸ¤ **Flujo de Trabajo Recomendado:**

1. **Ciprian** crea modelos base en su rama
2. **Barb** toma esos modelos y los optimiza con este framework
3. **IntegraciÃ³n** de resultados en la rama principal

## ğŸ“š **DocumentaciÃ³n Adicional:**

- [Scikit-learn Cross-validation](https://scikit-learn.org/stable/modules/cross_validation.html)
- [GridSearchCV Documentation](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html)
- [Model Evaluation Metrics](https://scikit-learn.org/stable/modules/model_evaluation.html)

---

