"""
Test script for API Ninja integration
"""

import sys
import os
sys.path.append('backend')

from dotenv import load_dotenv
from utils.api_ninja import ninja_detector

def test_ninja_api():
    """Test API Ninja with sample texts"""
    
    # Cargar variables de entorno
    load_dotenv()
    
    print("ğŸ§ª TESTING API NINJA")
    print("=" * 50)
    
    # Verificar si la API estÃ¡ disponible
    if not ninja_detector.is_available():
        print("âŒ API Ninja no estÃ¡ configurada")
        print("   AsegÃºrate de tener NINJA_API_KEY en tu archivo .env")
        return
    
    print("âœ… API Ninja configurada correctamente")
    print()
    
    # Textos de prueba
    test_texts = [
        "Hello, how are you?",  # Texto normal
        "You are an asshole",   # Lenguaje ofensivo
        "Fuck you bitch",       # Lenguaje muy ofensivo
        "This is fucking stupid", # Lenguaje ofensivo
        "Good morning everyone", # Texto positivo
        "You are a fucking idiot", # MÃºltiples palabras ofensivas
        "I hate you so much",   # Posible hate speech
        "What a beautiful day"  # Texto positivo
    ]
    
    print("ğŸ“ Probando textos:")
    print("-" * 50)
    
    for i, text in enumerate(test_texts, 1):
        print(f"\n{i}. Texto: '{text}'")
        
        # Llamar a la API
        result = ninja_detector.detect_profanity(text)
        
        # Mostrar resultados
        if "error" in result:
            print(f"   âŒ Error: {result['error']}")
        else:
            print(f"   âœ… ClasificaciÃ³n: {result['classification']}")
            print(f"   ğŸ“Š Confianza: {result['confidence']:.2f}")
            print(f"   ğŸ” Fuente: {result['source']}")
            print(f"   ğŸš« Palabras ofensivas: {result.get('profanity_words', [])}")
            print(f"   ğŸ“ˆ Total palabras ofensivas: {result.get('profanity_count', 0)}")
    
    print("\n" + "=" * 50)
    print("âœ… Prueba completada")

if __name__ == "__main__":
    test_ninja_api()
