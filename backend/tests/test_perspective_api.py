"""
Test script for Google Perspective API integration
"""

import sys
import os
sys.path.append('backend')

from dotenv import load_dotenv
from utils.api_perspective import perspective_detector

def test_perspective_api():
    """Test Google Perspective API with sample texts"""
    
    # Cargar variables de entorno
    load_dotenv()
    
    print("ğŸ§ª TESTING GOOGLE PERSPECTIVE API")
    print("=" * 50)
    
    # Verificar si la API estÃ¡ disponible
    if not perspective_detector.is_available():
        print("âŒ Google Perspective API no estÃ¡ configurada")
        print("   AsegÃºrate de tener PERSPECTIVE_API_KEY en tu archivo .env")
        return
    
    print("âœ… Google Perspective API configurada correctamente")
    print()
    
    # Textos de prueba
    test_texts = [
        "Hello, how are you?",  # Texto normal
        "You are an asshole",   # Lenguaje ofensivo
        "Fuck you bitch",       # Lenguaje muy ofensivo
        "This is fucking stupid", # Lenguaje ofensivo
        "Good morning everyone", # Texto positivo
        "You are a fucking idiot", # MÃºltiples palabras ofensivas
        "I hate you so much",   # Posible hate speech
        "What a beautiful day",  # Texto positivo
        "You are a faggot",     # Hate speech
        "I will kill you"       # Threat
    ]
    
    print("ğŸ“ Probando textos:")
    print("-" * 50)
    
    for i, text in enumerate(test_texts, 1):
        print(f"\n{i}. Texto: '{text}'")
        
        # Llamar a la API
        result = perspective_detector.detect_toxicity(text)
        
        # Mostrar resultados
        if "error" in result:
            print(f"   âŒ Error: {result['error']}")
        else:
            print(f"   âœ… ClasificaciÃ³n: {result['classification']}")
            print(f"   ğŸ“Š Confianza: {result['confidence']:.2f}")
            print(f"   ğŸ” Fuente: {result['source']}")
            print(f"   ğŸš« Tipo principal: {result.get('main_toxicity_type', 'N/A')}")
            print(f"   ğŸ“ˆ Score principal: {result.get('main_toxicity_score', 0):.2f}")
            print(f"   ğŸ¯ Toxicidad: {result.get('toxicity_score', 0):.2f}")
            print(f"   âš ï¸  Insulto: {result.get('insult', 0):.2f}")
            print(f"   ğŸ’€ Amenaza: {result.get('threat', 0):.2f}")
    
    print("\n" + "=" * 50)
    print("âœ… Prueba completada")

if __name__ == "__main__":
    test_perspective_api()
